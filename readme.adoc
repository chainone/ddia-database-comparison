[cols="10*", options="header"]
|===

2+| Distributed Databases/message queues | MySQL | HBase | Cassandra | DynamoDB | MongoDB | ElasticSearch  | Kafka | RabbitMQ

.7+| Replication

| Write
| Single write to leader
a|[.small]
Single write to the target region server
First write needs lookup (location of meta table) in zk first and then the region server that holds meta table and then do the actual write
meta table could be cached in client

a|[.small]
Multi-write, select any node as coordinator node and forward request to the nodes hold the target data
| ?
a|[.small]
Single write to the primary node of the target Replica Set  and then async to secondaries,
reads from secondaries may return data that does not reflect the state of the data on the primary
| ?
| Y
| ?



| Replication Type
a|[.small]
Single leader, all data replication
a|[.small]
Partition based on key range, write to single region server, replica via HDFS

a|[.small]
Leaderless, partition, multi-writes to selected nodes and with read repair if stable value is obtained

| Leaderless
a|[.small]
leader-based in one replica set, all data on primary need to be synced to secondaries
| ?
| leader-based
| leader-based






| Sync/Async Replication
a|[.small]
Configurable, default semi-sync
a|[.small]
Kinda SYNC, Write to WAL(persistent in HDFS) and in member store and then return
a|[.small]
Configurable, the w in quorum ((w + r > n)
| ?
| Async
| ?
| Y
| Y




| FailureOver
a|[.small]

* Follower: Follower copies latest snapshot from leader and then catch up with binlog
* Leader: Followers will most recent binlog becomes leader, old leader's unreplicated writes are simply discarded

a|[.small]
* ZK: whole cluster is down
* Master: need in-active master as hot backup, otherwise DDL operations will fail
* Region Servers: node recover from WAL, while recovering, that part of data is not available

a|[.small]
NO IMPACT as long as quorum ((w + r > n)) is satisfied

a|[.small]
Should be same as Cassandra
a|[.small]
* Primary: eligible secondary calls for an election to nominate itself as the new primary. The replica set cannot process write operations until the election completes successfully.
The replica set can continue to serve read queries if such queries are configured to run on secondaries while the primary is offline.
* Secondary: first copy all data from one of the members, and then apply all changes by syncing oplog

| Y
| Y
| ?








| Replication Topology
a|[.small]
Circular by default
| NA
a|[.small]
Circular
| Y
a|[.small]
No fixed topology, Secondary (Follower) chooses to sync oplog from Primary or Secondary based on ping time and the state of other secondary's replica status
| Y
| Y
| Y



| Replication Logs
a|[.small]
Originally STATEMENT-BASED, default to LOGICAL(row-based) if any nondeterminism in statement
| WAL
a|[.small]
Commit Log, just like the WAL in HBase, however, the write doesn't wait for finishing writing to in-memory store
| Y
a|[.small]
Op Log, should be STATEMENT-BASED with transforms
| ?
| Y
| Y



| Multi-Write Conflict Resolve
a|[.small]
NA (as all writes are sent to leader)
a|[.small]
NA (as writes are region-based, no conflict)
a|[.small]
LWW (last write win)
| Y
a|[.small]
NA (as write are shard(partition) based, no conflict)
| ?
| Y
| Y




.4+| Partition
| Partitioning Strategy
| NA
| Key Range
a|[.small]
First Key Hash, left key range
| ?
a|[.small]
Key range before 2.4, hash and range key both support later on
| ?
| Y
| ?



| Secondary Indexes
| NA
| No secondary index by default
| Local
| Global(term-partitioned)
| Local
| Local
| ?
| ?




| Rebalancing Strategy
| NA
| Dynamic Partitioning
a|[.small]
Partitioning proportionally to nodes, move split partitions between
| ?
a|[.small]
Number of partitions equals to that of replica sets, one partition has a lot of 64MB-size chunks,
partitions could be added later one and the number of chunks will be re-balanced across partitions (shards)

a|[.small]
Fixed number of partitions per index, entire partitions moved between nodes

| Kafka
| RabbitMQ






| Request Routing
| NA
a|[.small]
Routing Tier(ZK), if no cache on client, meta table looking-up in zk first and then the region server is required
meta table could be cached in client
a|[.small]
Client request to any node and then forward if miss
| DynamoDB
| Routing Tier (multiple mongos to route and aggregate, and one config server to store data location information(on which partition))
| ES
a|[.small]
? Routing Tier(or partition aware client?)(ZK)
| RabbitMQ
|===



## Reference
[1] ddia book
[2] MongoDB: the definitive guide
[3] MongoDB Manual
